{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_assign1_LinReg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xmaZgnfYuo_"
      },
      "source": [
        "# imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from datetime import datetime as dt\n",
        "print(dt.now())\n",
        "\n",
        "# https://github.com/sastaffo/CSU44061/blob/master/linearRegression/linreg.py\n",
        "\n",
        "# https://www.scss.tcd.ie/Doug.Leith/CSU44061/week1.php\n",
        "# id:9--4517.1-9 \n",
        "\n",
        "#############################################################################################\n",
        "\n",
        "def readEntries(file_location):\n",
        "  # reading in data (and checking first few entries)\n",
        "  df = pd.read_csv (file_location, comment=\"#\")\n",
        "  print(df.head())\n",
        "\n",
        "  X = np.array( df.iloc[:,0]).reshape(-1,1)\n",
        "  print(X[0:5])\n",
        "  y = np.array( df.iloc[:,1]).reshape(-1,1)\n",
        "  print(y[0:5])\n",
        "\n",
        "  print(X.size, y.size)\n",
        "\n",
        "  return (X,y)\n",
        "#end readEntries()\n",
        "\n",
        "\n",
        "print(\"\\ndef readEntries(file_location)\\n\", dt.now())\n",
        "# this is for some reason, leaving out the 1st row on the table [0, -509.8...]\n",
        "\n",
        "# (X,y) = readEntries(\"https://raw.githubusercontent.com/johnsl01/ML2020A/main/ML_week1.csv\")\n",
        "\n",
        "#############################################################################################\n",
        "\n",
        "\n",
        "def plotXY(X, y, xlabel, ylabel, legend):\n",
        "  # takes in X and y and plots them\n",
        "  plt.rc(\"font\", size=18)\n",
        "  plt.rcParams[\"figure.constrained_layout.use\"] = True\n",
        "  plt.scatter(X, y, color=\"black\", linewidth=0.5)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.legend(legend)\n",
        "  plt.show()\n",
        "#end plotXY()\n",
        "print(\"\\ndef plotXY(X,y)\\n\", dt.now())\n",
        "\n",
        "\n",
        "def plotXYLine(X, y, ypred, xlabel, ylabel, legend):\n",
        "  plt.rc(\"font\", size=18)\n",
        "  plt.rcParams[\"figure.constrained_layout.use\"] = True\n",
        "  plt.scatter(X, y, color=\"black\", linewidth=0.5)\n",
        "  plt.plot(X, ypred, color=\"blue\", linewidth=1)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.legend(legend)\n",
        "  plt.show()\n",
        "#end plotXYLine\n",
        "print(\"\\ndef plotXYLine(X,y,ypred,xlabel,ylabel,legend)\\n\", dt.now())\n",
        "\n",
        "\n",
        "#############################################################################################\n",
        "\n",
        "\n",
        "def normaliseData(X):\n",
        "  # rescale data to lie between 0 and 1\n",
        "  maxX = X.max(axis=0)\n",
        "  minX = X.min(axis=0)\n",
        "  print(\"max=\", maxX, \"\\nmin=\",minX)\n",
        "  normX = (X - minX)/(maxX - minX)\n",
        "  print(\"new max=\", normX.max(axis=0), \"new min=\", normX.min(axis=0))\n",
        "  print(X[0:5])\n",
        "  print(normX[0:5])\n",
        "  return (normX)\n",
        "# end normaliseData()\n",
        "print(\"\\ndef normaliseData(X)\\n\", dt.now())\n",
        "\n",
        "\n",
        "#############################################################################################\n",
        "\n",
        "\n",
        "def predict(X, theta):\n",
        "  # takes m by n matrix X as input and returns an m by 1 vector containing the\n",
        "  # predictions h_theta(x^i) for each row x^i, i=1,...,m in X\n",
        "  t=len(theta)\n",
        "  if len(X.shape) is 1: # ie if there is only 1 data point input\n",
        "    pred = X[0]*theta[0]\n",
        "    for i in range(1,t) :\n",
        "      pred = pred + X[i]*theta[i]\n",
        "    # end for\n",
        "  else: # ie if there are multiple data points in X array\n",
        "    pred = X[:,0]*theta[0] + (X[:,1]*theta[1])\n",
        "  return pred\n",
        "# end predict\n",
        "print(\"\\ndef predict(X, theta)\\n\", dt.now())\n",
        "\n",
        "\n",
        "def computeCost(X,y,theta):\n",
        "  # calculates the cost of J(theta) and returns cost\n",
        "  pred = predict(X,theta)\n",
        "  sqdiff = (pred-y)**2\n",
        "  cost = (sqdiff.sum())/(2*len(y))\n",
        "  return cost\n",
        "#end computeCost()\n",
        "print(\"\\ndef computeCost(X, y, theta)\\n\", dt.now())\n",
        "\n",
        "\n",
        "def computeGradientA(X, y, theta):\n",
        "  # function calulate the gradient of J(theta) and returns its value\n",
        "  grad = np.zeros(2)\n",
        "  costpred = predict (X,theta)\n",
        "  costbase = costpred - y\n",
        "  costprodb0 = costbase #  X[:,0] is all ones so don't multiply\n",
        "  costprodb1 = costbase * X[:,1]\n",
        "  costprodb0sum = costprodb0.sum()\n",
        "  costprodb1sum = costprodb1.sum()\n",
        "  grad[0] = costprodb0sum / len(y)\n",
        "  grad[1] = costprodb1sum / len(y)\n",
        "  return grad\n",
        "# end def computeGradient\n",
        "print(\"\\ndef computeGradientA(X, y, theta)\")\n",
        "\n",
        "\n",
        "def computeGradientB(X, y, theta):\n",
        "  # function calulate the gradient of J(theta) and returns its value\n",
        "  # for X with 1 feature\n",
        "  grad = np.zeros(2)\n",
        "  # print(\"pre-prediction X[:10]=\",X[:10], \"\\ntheta=\",theta)\n",
        "  pred = predict(X, theta)\n",
        "  # print(\"pred=\",pred)\n",
        "  diff = pred - y\n",
        "  prod = diff * X[:,1]\n",
        "  grad[0] = diff.sum() / len(y)\n",
        "  grad[1] = prod.sum() / len(y)\n",
        "  # print(\"grad=\",grad)\n",
        "  return grad\n",
        "# end computeGradient\n",
        "print(\"\\ndef computeGradB(X, y, theta)\\n\", dt.now())\n",
        "\n",
        "\n",
        "def gradientDescent(X, y, alpha, iters):\n",
        "  # iteratively update parameter vector theta\n",
        "  cost = np.zeros(iters)\n",
        "  (_,n) = X.shape\n",
        "  theta= np.zeros(n)\n",
        "  print(\"theta on init=\",theta)\n",
        "  for i in range(iters):\n",
        "    theta = theta - alpha * computeGradient(X,y,theta)\n",
        "    # print(\"theta [i=\",i,\"]=\",theta)\n",
        "    cost[i] = computeCost(X, y, theta)\n",
        "  #end for\n",
        "  return theta, cost\n",
        "#end gradientDescent(X, y, numparams, alpha, iters)\n",
        "print(\"\\ndef gradientDescent(X, y, alpha, iters)\\n\", dt.now())\n",
        "\n",
        "\n",
        "#############################################################################################\n",
        "\n",
        "\n",
        "def h_theta(X, theta):\n",
        "  print(\"theta=\",theta)\n",
        "  print(\"X[:10]=\\n\", X[:10])\n",
        "  (_,n) = X.shape\n",
        "  t = len(theta)\n",
        "  if (t is n+1):\n",
        "    theta_np = np.array(theta[1:(t)])\n",
        "    theta0 = theta[0]\n",
        "  elif (t is n):\n",
        "    theta_np = np.array(theta[0:(t)])\n",
        "    theta0 = 0\n",
        "  #end if\n",
        "  print(\"theta0=  \", theta0)\n",
        "  print(\"theta_np=\", theta_np)\n",
        "\n",
        "  # h_theta_x = theta[0] + theta[1]*X[1] + ...\n",
        "  h_theta_x = np.multiply(theta_np, X) # multiples all theta[i]*X[i], excluding theta[0] for now\n",
        "  print(\"\\nh_theta_x[:10]=\\n\", h_theta_x[:10])\n",
        "\n",
        "  h_theta_x = np.add(h_theta_x, theta0) # sums up all theta[i]*X[i], adding theta[0]\n",
        "  print(\"\\nh_theta_x[:10]=\\n\", h_theta_x[:10])\n",
        "  return h_theta_x\n",
        "#end h_theta\n",
        "print(\"\\ndef h_theta(X, theta)\\n\", dt.now())\n",
        "\n",
        "\n",
        "def linearRegressionTrain(X, y, theta):\n",
        "  # returns predictions for X given theta and j_theta [(sum of sq diffs)/2*m]\n",
        "  (m,_) = X.shape\n",
        "  ypred = h_theta(X,theta)\n",
        "  diff = np.subtract(ypred, y)\n",
        "  sumsqdiff = np.sum(np.multiply(diff, diff))\n",
        "  j_theta = sumsqdiff/(2*m)\n",
        "  return (ypred,j_theta)\n",
        "print(\"\\ndef linearRegressionTrain(X, y, theta)\\n\", dt.now())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\", dt.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K-VB5MqhdYZ"
      },
      "source": [
        "def main():\n",
        "  (X,y) = readEntries(\"https://raw.githubusercontent.com/johnsl01/ML2020A/main/ML_week1.csv\")\n",
        "  plotXY(X, y, \"input X\", \"output y\", [\"training data\"])\n",
        "\n",
        "  normX = normaliseData(X)\n",
        "  (m,n) = X.shape\n",
        "  newX = np.ones((m,n+1))\n",
        "  newX[:,1] = normX[:,0]\n",
        "  print(\"newX[:10]\\n\", newX[:10])\n",
        "\n",
        "  \n",
        "  normY = normaliseData(y)\n",
        "  plotXY(newX[:,1], normY, \"input X (normalised)\", \"output y (normalised)\", [\"training data\"])\n",
        "\n",
        "  iters = 5\n",
        "  (theta, cost) = gradientDescent(newX, normY, 0.5, iters)\n",
        "  itersX = np.arange(0,iters)\n",
        "  #plotXY(itersX, cost, \"iteration#\", \"cost\", [\"cost per iteration\"])\n",
        "\n",
        "  print(\"theta.shape=\", theta.shape)\n",
        "  print(\"theta=\", theta)\n",
        "  print(\"normX.shape=\", normX.shape)\n",
        "  (predY, j_theta) = linearRegressionTrain(normX,normY,theta)\n",
        "  print(\"predY.shape=\", predY.shape)\n",
        "  print(\"predY[:10]=\\n\",  predY[:10])\n",
        "\n",
        "  \n",
        "  #plotXYLine(normX[:,1], normY, predY, \"input X\", \"output y\", [\"predictions\",\"training data\"])\n",
        "\n",
        "  plt.rc(\"font\", size=18)\n",
        "  plt.rcParams[\"figure.constrained_layout.use\"] = True\n",
        "  plt.scatter(newX[:,1], normY, color=\"black\", linewidth=0.5)\n",
        "  plt.plot(newX[:,1], predY[:,0], color=\"blue\", linewidth=1)\n",
        "  plt.xlabel(\"input X\")\n",
        "  plt.ylabel(\"output Y\")\n",
        "  plt.legend([\"predictions\",\"training data\"])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# end main\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}