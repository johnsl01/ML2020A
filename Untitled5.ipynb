{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQLgb8B2yLQN5n15tnaMpL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnsl01/ML2020A/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzjAbaFzUhw7"
      },
      "source": [
        "# ALL IN ONE\n",
        "\n",
        "# id:9--4517.1-9\n",
        "\n",
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def readEntries(file_location):\n",
        "  # reading in data (and checking first few entries)\n",
        "  data = pd.read_csv (file_location, comment=\"#\")\n",
        "  X = np.array( data.iloc[:,0]).reshape(-1,1)\n",
        "  y = np.array( data.iloc[:,1]).reshape(-1,1)\n",
        "  return (X,y)\n",
        "#end readEntries\n",
        "\n",
        "\n",
        "def plotXY(X, y, xlabel, ylabel, legend, title):\n",
        "  # takes in X and y and plots them\n",
        "  plt.rc(\"font\", size=18)\n",
        "  plt.rcParams[\"figure.constrained_layout.use\"] = True\n",
        "  plt.scatter(X, y, color=\"black\", linewidth=0.5)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.legend(legend)\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "#end plotXY()\n",
        "\n",
        "def plotXYpredict(X,y,predY,alpha,iters):\n",
        "  plt.rc(\"font\", size=18)\n",
        "\n",
        "  ### False !\n",
        "  plt.rcParams[\"figure.constrained_layout.use\"] = False\n",
        "  plt.scatter(X, y, color=\"black\", linewidth=0.5)\n",
        "  plt.plot(X, predY, color=\"yellow\", linewidth=3)\n",
        "  plt.xlabel(\"input X (normalised)\")\n",
        "  plt.ylabel(\"output Y\")\n",
        "  plt.legend([\"predictions\",\"training data\"])\n",
        "  title = (\"Prediction for Y where alpha=\", alpha, \" and iters=\", iters)\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "#end plotXYpredict\n",
        "\n",
        "def normaliseData(X):\n",
        "  # rescale data to lie between 0 and 1\n",
        "\n",
        "  scale = X.max(axis=0)\n",
        "  return (X/scale, scale)\n",
        "  '''\n",
        "  maxX = X.max(axis=0)\n",
        "  minX = X.min(axis=0)\n",
        "  normX = (X - minX)/(maxX - minX)\n",
        "  scaleX = maxX - minX\n",
        "  return (normX, scaleX)\n",
        "  '''\n",
        "# end normaliseData()\n",
        "\n",
        "\n",
        "def predict(X, theta):\n",
        "  # takes m by n matrix X as input and returns an m by 1 vector containing the\n",
        "  # predictions h_theta(x^i) for each row x^i, i=1,...,m in X\n",
        "  # for X with 1 feature and a ONES column\n",
        "  pred = X[:,0]*theta[0] + (X[:,1]*theta[1])\n",
        "  # print (\"Thetas : \",theta[0] ,theta[1])\n",
        "  # print (\"pred [:10] : \",pred [:10])\n",
        "  return pred\n",
        "# end predict\n",
        "\n",
        "\n",
        "'''\n",
        "def computeCost(X,y,theta):\n",
        "  # calculates the cost of J(theta) and returns cost\n",
        "  pred = predict(X,theta)\n",
        "  sqdiff = (pred-y)**2\n",
        "  sum = sqdiff.sum()\n",
        "  cost = (sum)/(2*len(y))\n",
        "  print (\"Compute Cost #7 :\", cost)  \n",
        "  return cost\n",
        "#end computeCost()\n",
        "'''\n",
        "\n",
        "print(\"def computeCost(X, y, theta)\")\n",
        "def computeCost(X, y, theta):\n",
        "\n",
        "  # print (\"ComputeCost : \", X[:3,0], X[:3,1], y[:3], theta)\n",
        "  # print (X.shape, y.shape, theta.shape)\n",
        "  # print (type(X), type(y), type(theta))\n",
        "\n",
        "  n = (len(y))\n",
        "  z = y.reshape(n,)\n",
        "\n",
        "  # print (z.shape)\n",
        "\n",
        "  # function calculates the cost J(theta) and return its value\n",
        "  ##### replace the next line with your code #####\n",
        "  \n",
        "  # This function is already independent of the number of variables\n",
        "  #   provided the X array carries the variables with a leading 1 on data point(s)\n",
        "  #   and the theta has the same number of B0, B1, B2 ...  coeficients.\n",
        "  # However the predict it calls needs to be able to deal with the lenght of the theta \n",
        "  #   which corresponds to the number of variables + 1\n",
        "\n",
        "\n",
        "  # The sequential calculation could be done in fewer steps\n",
        "  # which would be more memory efficient but less clear.\n",
        "\n",
        "  # TODO  merge some calculations to tidy up the code and add efficiency\n",
        "  \n",
        "  # print (\"Compute Cost #1 : \" , X.shape, y.shape, theta.shape)\n",
        "\n",
        "  # Get the value predicted by the current theta values\n",
        "  costpred = predict (X,theta)\n",
        "  # print (\"Compute Cost #2 :\", costpred.shape)\n",
        "\n",
        "  # get the difference between the predictions and the actuals\n",
        "  costbase = costpred - z\n",
        "  # print (\"Compute Cost #3 :\", costbase.shape)\n",
        "\n",
        "  # get the square of the differences\n",
        "  costsq = costbase**2\n",
        "  # print (\"Compute Cost #4 :\", costsq.shape)\n",
        "\n",
        "  # get the sum of the squared differences\n",
        "  sumcost = costsq.sum()\n",
        "  # print (\"Compute Cost #5 :\", sumcost)\n",
        "\n",
        "  # print (y)\n",
        "  # print (costpred)\n",
        "  # print (\"Compute Cost #6 :\", len(y))  \n",
        "\n",
        "  # divide the sum of squares by twice the number of data points\n",
        "  cost = (sumcost)/(2*len(z))\n",
        "  # print (\"Compute Cost #7 :\", cost)  \n",
        "\n",
        "  # return the cost of the current theta values\n",
        "  return cost\n",
        "# end def computeCost\n",
        "\n",
        "\n",
        "\n",
        "def computeGradient(X, y, theta):\n",
        "  # function calulate the gradient of J(theta) and returns its value\n",
        "  # for X with 1 feature\n",
        "  grad = np.zeros(2)\n",
        "  pred = predict(X, theta)\n",
        "  diff = np.subtract(pred, y[:,0])\n",
        "  prod = diff * X[:,1]\n",
        "  grad[0] = diff.sum() / len(y) # don't need to multiply by X[:,0] since it's all 1s\n",
        "  grad[1] = prod.sum() / len(y)\n",
        "  return grad\n",
        "# end computeGradient\n",
        "\n",
        "\n",
        "\n",
        "def gradientDescent(X, y, alpha, iters):\n",
        "  # iteratively update parameter vector theta\n",
        "  cost = np.zeros(iters)\n",
        "  (_,n) = X.shape\n",
        "  theta= np.zeros(n)\n",
        "  for i in range(iters):\n",
        "    ###\n",
        "    # sub = alpha * computeGradient(X,y,theta)\n",
        "    # theta = theta - sub\n",
        "    ### checking shape are correct\n",
        "    theta = theta - alpha * computeGradient(X,y,theta) \n",
        "\n",
        "\n",
        "    cost[i] = computeCost(X, y, theta)\n",
        "\n",
        "\n",
        "  #end for\n",
        "  return theta, cost\n",
        "#end gradientDescent(X, y, numparams, alpha, iters)\n",
        "\n",
        "\n",
        "def h_theta(X, theta):\n",
        "  # calculates h_theta_x for a given X and theta\n",
        "  (m,n) = X.shape\n",
        "  t = len(theta)\n",
        "  if (t is n+1):\n",
        "    theta_np = np.array(theta[1:(t)])\n",
        "    theta0 = theta[0]\n",
        "  elif (t is n):\n",
        "    theta_np = np.array(theta[0:(t)])\n",
        "    theta0 = 0\n",
        "  #end if\n",
        "\n",
        "  # h_theta_x = theta[0] + theta[1]*X[1] + ...\n",
        "  h_theta_x_mul = np.multiply(theta_np, X) # multiples all theta[i]*X[i], excluding theta[0] for now\n",
        "  h_theta_x = np.ones((m))\n",
        "  for i in range(0,m):\n",
        "    h_theta_x[i] = np.sum(h_theta_x_mul[i]) + theta0 # sums up all theta[1]*X[1] along each row, adding theta0\n",
        "  #end for\n",
        "  return h_theta_x\n",
        "#end h_theta\n",
        "\n",
        "\n",
        "def linearRegressionTrain(X, y, theta):\n",
        "  # returns predictions for X given theta and j_theta [(sum of sq diffs)/2*m]\n",
        "  (m,_) = X.shape\n",
        "  ypred = h_theta(X,theta)\n",
        "  diff = np.subtract(ypred, y)\n",
        "  sumsqdiff = np.sum(np.multiply(diff, diff))\n",
        "  j_theta = sumsqdiff/(2*m)\n",
        "  return (ypred,j_theta)\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "  # data=pd.read_csv('/content/ML_week1.csv', comment='#')\n",
        "  ### J file\n",
        "  data=pd.read_csv('https://raw.githubusercontent.com/johnsl01/ML2020A/main/ML_week1.csv', comment='#', delimiter=',')\n",
        "\n",
        "  X=np.array(data.iloc[:,0])\n",
        "  X=X.reshape(-1,1)\n",
        "  y=np.array(data.iloc[:,1])\n",
        "  y=y.reshape(-1,1)\n",
        "  '''\n",
        "  plotXY(X, y, \"input X\", \"output y\", [\"training data\"]\n",
        "         , \"plotting X Y where X is not normalised\")\n",
        "  '''\n",
        "  normX, scaleX = normaliseData(X)\n",
        "  (m,n) = X.shape\n",
        "  newX = np.ones((m,n+1))\n",
        "  newX[:,1] = normX[:,0]  # for X with 1 feature!\n",
        "   #print(\"newX[:10]\\n\", newX[:10])\n",
        "\n",
        "  y, scaleY = normaliseData(y)\n",
        "  '''\n",
        "  plotXY(newX[:,1],  y, \"input X (normalised)\", \"output y\", [\"training data\"]\n",
        "         , \"plotting X Y where X is normalised\")\n",
        "  '''\n",
        "  ### fiddling with iters \n",
        "  iters = 500\n",
        "  itersX = np.arange(0,iters)\n",
        "\n",
        "  alpha_1 = 0.5\n",
        "  (theta_1, cost_1) = gradientDescent(newX, y, alpha_1, iters)\n",
        "  print (\"head (newX) : \", newX[:10,0], newX[:10,1])\n",
        "  print (\"head (y) : \", y[:10])\n",
        "  print (\"shape : \",cost_1.shape)\n",
        "  print (\"costs [:10] : \",cost_1[:10])\n",
        "  (predY_1, j_theta_1) = linearRegressionTrain(newX, y,theta_1)\n",
        "  print(\"\\nalpha=\", alpha_1)\n",
        "  print(\"theta=\",theta_1)\n",
        "  print(\"j_theta=\",j_theta_1)\n",
        "  plotXYpredict(newX[:,1],y,predY_1,alpha_1,iters)\n",
        "\n",
        "  \n",
        "  alpha_2 = 0.3\n",
        "  (theta_2, cost_2) = gradientDescent(newX,  y, alpha_2, iters)\n",
        "  (predY_2, j_theta_2) = linearRegressionTrain(newX, y,theta_2)\n",
        "  print(\"\\nalpha=\", alpha_2)\n",
        "  print(\"theta=\",theta_2)\n",
        "  print(\"j_theta=\",j_theta_2)\n",
        "  plotXYpredict(newX[:,1], y,predY_2,alpha_2,iters)\n",
        "\n",
        "  alpha_3 = 0.2\n",
        "  (theta_3, cost_3) = gradientDescent(newX, y, alpha_3, iters)\n",
        "  (predY_3, j_theta_3) = linearRegressionTrain(newX, y,theta_3)\n",
        "  print(\"\\nalpha=\", alpha_3)\n",
        "  print(\"theta=\",theta_3)\n",
        "  print(\"j_theta=\",j_theta_3)\n",
        "  plotXYpredict(newX[:,1], y,predY_3,alpha_3,iters)\n",
        "  \n",
        "  tailN = 250\n",
        "  tailX = np.arange(iters - tailN, iters)\n",
        "  # print(tailX)\n",
        "\n",
        "  tailCost_1 = cost_1[-tailN:]\n",
        "  # print(tailCost_1)\n",
        "  tailCost_2 = cost_2[-tailN:]\n",
        "  tailCost_3 = cost_3[-tailN:]\n",
        "\n",
        "  # plots 3 cost functions for 3 different alphas\n",
        "  plt.rc(\"font\", size=18)\n",
        "  plt.rcParams[\"figure.constrained_layout.use\"] = True\n",
        "  plt.scatter(itersX, cost_1, color=\"black\", linewidth=0.1)\n",
        "  plt.scatter(itersX, cost_2, color=\"red\", linewidth=0.1)\n",
        "  plt.scatter(itersX, cost_3, color=\"green\", linewidth=0.1)\n",
        "  plt.xlabel(\"iteration#\")\n",
        "  plt.ylabel(\"cost\")\n",
        "  ### don't hard code !\n",
        "  plt.legend([\"alpha = \"+str(alpha_1), \"alpha = \"+str(alpha_2), \"alpha = \"+str(alpha_3)])\n",
        "  # plt.legend([\"alpha = \"+str(alpha_1)])\n",
        "  plt.show()\n",
        "\n",
        "    # plots 3 cost functions for 3 different alphas (tail ends)\n",
        "  plt.rc(\"font\", size=18)\n",
        "  plt.rcParams[\"figure.constrained_layout.use\"] = True\n",
        "  plt.scatter(tailX, tailCost_1, color=\"black\", linewidth=0.1)\n",
        "  plt.scatter(tailX, tailCost_2, color=\"red\", linewidth=0.1)\n",
        "  plt.scatter(tailX, tailCost_3, color=\"green\", linewidth=0.1)\n",
        "  plt.xlabel(\"iteration#\")\n",
        "  plt.ylabel(\"cost\")\n",
        "  ### don't hard code !\n",
        "  plt.legend([\"alpha = \"+str(alpha_1), \"alpha = \"+str(alpha_2), \"alpha = \"+str(alpha_3)])\n",
        "  # plt.legend([\"alpha = \"+str(alpha_1)])\n",
        "  plt.show()\n",
        "# end main\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}